{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### authors: Rafael Dousse, Eva Ray, Massimo Stefani",
   "id": "b7713918a33c8eb"
  },
  {
   "cell_type": "markdown",
   "id": "d6c0f394",
   "metadata": {},
   "source": [
    "# Exercise 3 - Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330e2ec",
   "metadata": {},
   "source": [
    "### a) Why do we have a gradient ascent in the case of logistic regression while we had a gradient descent with linear regression ? Can we convert the gradient ascent of logistic regression into a gradient descent ? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d69de",
   "metadata": {},
   "source": [
    "In the cas of a logistic regression, we have a gradient ascent because we are trying to maximize the objective function, which is the likelihood of the observed data given the model parameters. In contrast, in linear regression, we minimize the cost function using gradient descent.\n",
    "\n",
    "Yes, we can convert the gradient ascent of logistic regression into a gradient descent by minimizing the negative of the objective function instead of maximizing the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9000c86",
   "metadata": {},
   "source": "### b) Assuming a logistic regression with a linear decision boundary taking as input samples in two dimensions (x1; x2), in which case do we get 0,5 as output of the classification system ? Express your answer with an equation."
  },
  {
   "cell_type": "markdown",
   "id": "3adfff97",
   "metadata": {},
   "source": [
    "To have an output of 0.5, we must have the following equation: $0.5 = 1 / (1 + exp(-(θ_0 + θ_1*x1 + θ_2*x2)))$. We know that the sigmoid function outputs 0.5 when its input is 0. Therefore, we need to solve the equation:\n",
    "$θ_0 + θ_1*x1 + θ_2*x2 = 0$. This equation represents the decision boundary where the predicted probability is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d782c",
   "metadata": {},
   "source": [
    "### c) What is the computational trick to avoid numerical problems in the computation of J(theta) for the logistic regression ? In which situations (for what type of inputs) do we risk to observe such numerical problems ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e405c9",
   "metadata": {},
   "source": [
    "We may observe numerical problems during the computation of $J(theta$) because we can encounter $log(0)$. This can happen when the predicted probability is exactly 0 or 1, which can occur when the linear combination of inputs $(θ^T * x)$ is very large in magnitude (either positive or negative). To avoid these numerical problems, we can use a computational trick by adding a small constant (epsilon) to the predicted probabilities before taking the logarithm. This ensures that we never take the logarithm of zero and thus avoids numerical problems.\n",
    "\n",
    "The modified objective function becomes:\n",
    "\n",
    "$J(\\theta) = -\\frac{1}{N} \\sum_{n=1}^{N} \\left[ y_n \\log(h_\\theta(x_n) + \\epsilon) + (1 - y_n) \\log(1 - h_\\theta(x_n) + \\epsilon) \\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657c670",
   "metadata": {},
   "source": [
    "### d) A logistic regression can classify between 2 classes. How can we build a multi-class (with K classes) system with logistic regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9e92c",
   "metadata": {},
   "source": "To build a multi-class system, we can use the method called **one-vs-rest**. In this approach, we train K separate binary logistic regression classifiers, one for each class. Each classifier is trained to distinguish between one class and all the other classes combined. During prediction, we run all K classifiers on the input sample and select the class corresponding to the classifier that outputs the highest probability. This approach is actually very similar to using a NN with one hidden layer and using one-hot encoding for the target categories."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
