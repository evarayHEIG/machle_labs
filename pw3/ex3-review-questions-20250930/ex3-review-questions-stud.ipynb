{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Authors: Rafael Dousse, Eva Ray, Massimo Stefani",
   "id": "5aa2e6a5b3b11df2"
  },
  {
   "cell_type": "markdown",
   "id": "ad0d40d6",
   "metadata": {},
   "source": [
    "# Exercice 3 - Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e556a9d",
   "metadata": {},
   "source": [
    "**a) Assuming an univariate input *x*, what is the complexity at inference time of a Bayesian classifier based on histogram computation of the likelihood ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2fb7ef",
   "metadata": {},
   "source": [
    "The complexity is O(K) where K is the number of classes.\n",
    "\n",
    "For a univariate feature (D = 1) a histogram-based likelihood is a constant-time lookup per class (find the bin → O(1)), then multiply by the prior and compare. Doing that for all K classes gives O(K·1) = O(K). (If K and D are treated as constants and small, authors sometimes write this as ~O(1).) (Slide 26 from the week 3 lecture.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99632770",
   "metadata": {},
   "source": [
    "**b) Bayesian models are said to be generative as they can be used to generate new samples. Taking the implementation of the exercise 1.a, explain the steps to generate new samples using the system you have put into place.**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab64b2",
   "metadata": {},
   "source": [
    "The procedure is as follows:\n",
    "1. Draw a class C according to the prior distribution P(C).\n",
    "2. Draw a sample x from the likelihood distribution P(x|C) corresponding to the selected class C. In our case, we can use the histogram to determine the bin probabilities and sample accordingly.\n",
    "3. Repeat the above steps to generate as many samples as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f611fe",
   "metadata": {},
   "source": [
    "***Optional*: Provide an implementation in a function generateSample(priors, histValues, edgeValues, n)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14aba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generateSample(priors, histValues, edgeValues, n):\n",
    "    samples = []\n",
    "    classes = [0, 1]\n",
    "    for _ in range(n):\n",
    "        # Step 1: Draw a class C according to the prior distribution P(C).\n",
    "        C = np.random.choice(classes, p=priors)\n",
    "        # Step 2: Draw a sample x from the likelihood distribution P(x|C).\n",
    "        hist = histValues[C]\n",
    "        edges = edgeValues[C]\n",
    "        # Choose a bin according to the histogram probabilities\n",
    "        bin = np.random.choice(len(hist), p=hist/hist.sum())\n",
    "        # Uniformly sample within the bin\n",
    "        x = np.random.uniform(edges[bin], edges[bin + 1])\n",
    "        samples.append((x, C))\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c4f6b",
   "metadata": {},
   "source": [
    "**c) What is the minimum overall accuracy of a 2-class system relying only on priors and that is built on a training set that includes 5 times more samples in class A than in class B?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb03365",
   "metadata": {},
   "source": "We have P(B) = 5 * P(A) and P(A) + P(B) = 1, so P(A) = 1/6 and P(B) = 5/6. Since we only rely on priors, we will always predict the most probable class, which is class B. Therefore, the minimum overall accuracy is P(B) = 5/6."
  },
  {
   "cell_type": "markdown",
   "id": "58450ff6",
   "metadata": {},
   "source": [
    "**d) Let’s look back at the PW02 exercise 3 of last week. We have built a knn classification systems for images of digits on the MNIST database.**\n",
    "\n",
    "**How would you build a Bayesian classification for the same task ? Comment on the prior probabilities and on the likelihood estimators. More specifically, what kind of likelihood estimator could we use in this case ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf1500",
   "metadata": {},
   "source": [
    "To build a Bayesian classification system for the MNIST digit recognition task, follow these steps:\n",
    "1. **Prior Probabilities**: We estimate the prior probabilities P(C) for each digit class (0-9) based on the frequency of each class in the training dataset. Since the MNIST dataset is relatively balanced, we expect the priors to be roughly equal, but we still compute them from the training data.\n",
    "2. **Likelihood Estimators**: For the likelihood P(x|C), where x is the image data, we could use a histogram-based approach, but given the high dimensionality of image data, we might suffer from the curse of dimensionality. Instead, we use a Gaussian Naive Bayes approach, where we assume that the pixel values for each class are normally distributed. We estimate the mean and variance of the pixel values for each class from the training data. We use the log form of the Gaussian Naive Bayes approach to prevent float underflow when multiplying many small probabilities.\n",
    "3. **Classification**: For a new image, we compute the posterior probabilities for each class using Bayes' theorem and classify the image to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca9715",
   "metadata": {},
   "source": [
    "***Optional:* implement it and report performance !**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:49:58.661814Z",
     "start_time": "2025-10-05T14:49:58.378995Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class GaussianNaiveBayesMNIST:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.priors = np.array([np.mean(y == c) for c in self.classes])\n",
    "        # calculate each pixel mean and variance for each class\n",
    "        self.means = np.array([X[y == c].mean(axis=0) for c in self.classes])\n",
    "        self.vars = np.array([X[y == c].var(axis=0) + 1e-6 for c in self.classes])\n",
    "        # Adding a small value to variance to avoid division by zero\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_probs = []\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            mean = self.means[idx]\n",
    "            var = self.vars[idx]\n",
    "            prior = np.log(self.priors[idx])\n",
    "            # log-likelihood pixel-wise\n",
    "            log_likelihood = -0.5 * np.sum(np.log(2 * np.pi * var) + ((X - mean) ** 2) / var, axis=1)\n",
    "            log_probs.append(prior + log_likelihood)\n",
    "        return self.classes[np.argmax(log_probs, axis=0)]\n",
    "\n",
    "# Train and test\n",
    "model = GaussianNaiveBayesMNIST()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "```"
   ],
   "id": "934a3bbbe2f98bf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We ran this code in the notebook of PW02 exercise 3 and obtained an accuracy of 0.542. This is significantly lower than the KNN classifier. There could be several reasons for this performance difference:\n",
    "- The strong independence assumption of Naive Bayes may not hold for image data in MNIST dataset, where pixel values are often correlated.\n",
    "- The Gaussian assumption for pixel values may not accurately capture the true distribution of pixel intensities (0-255) in the images.\n",
    "- KNN can capture more complex decision boundaries by considering local neighborhoods, while Naive Bayes relies on global statistics."
   ],
   "id": "91b812c949965d4f"
  },
  {
   "cell_type": "markdown",
   "id": "b812b46f",
   "metadata": {},
   "source": [
    "**e) Read [europe-border-control-ai-lie-detector](https://theintercept.com/2019/07/26/europe-border-control-ai-lie-detector/). The described system is \"a virtual policeman designed to strengthen European borders\". It can be seen as a 2-class problem, either you are a suspicious traveler or you are not. If you are declared as suspicious by the system, you are routed to a human border agent who analyses your case in a more careful way.**\n",
    "\n",
    "1. What kind of errors can the system make ? Explain them in your own words.\n",
    "2. Is one error more critical than the other ? Explain why.\n",
    "3. According to the previous points, which metric would you recommend to tune your MLsystem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf1760",
   "metadata": {},
   "source": [
    "1. The system can make two types of errors: false positives (FP) and false negatives (FN).\n",
    "    - FP occurs when a non-suspicious traveler is incorrectly classified as suspicious, leading to unnecessary scrutiny and potential delays.\n",
    "    - FN occurs when a suspicious traveler is incorrectly classified as non-suspicious, allowing them to pass through the border without additional checks.\n",
    "2. A FN is more critical than a FP because it can lead to security risks, such as allowing individuals who may pose a threat to enter the country without proper screening. FPs only cause inconvenience to innocent travelers, which is less severe than the potential consequences of a FN.\n",
    "3. Our goal is to minimize the number of FNs (while keeping FPs at a reasonable level). Thus, we should focus on recall. A high recall ensures that most suspicious travelers are correctly identified, even if it means accepting a higher number of FPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a1f73-c0f7-4707-9551-c71bfa379960",
   "metadata": {},
   "source": [
    "**f) When a deep learning architecture is trained using an unbalanced training set, we usually observe a problem of bias, i.e. the system favors one class over another one. Using the Bayes equation, explain what is the origin of the problem.**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Remember that the Bayes equation is given by: P(C|x) = P(x|C) * P(C) / P(x)\n",
    "\n",
    "When the training set is unbalanced, the majority class has a higher prior probability P(C) than the minority class. As a result, the model tends to predict the majority class more often, since P(C) appears in the numerator of Bayes’ equation. This bias occurs because the model learns to associate features more strongly with the majority class due to its higher frequency in the training data."
   ],
   "id": "db822b30b99cfd4b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
