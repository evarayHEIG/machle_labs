# MachLe PW 10 - Report

#### authors: Rafael Dousse, Massimo Stefani, Eva Ray

## 1. Digit recognition from raw data

### First Model 

### Second Model

### Third Model

### Questions

> a. Select a neural network topology and describe the inputs, indicate how many are they, and how many outputs.

> b. Compute the number of weights of each model (e.g., how many weights between the input and the hidden layer, how many weights between each pair of layers, biases, etc..) and explain how you get to the total number of weights.

> c. Comment the differences in results for the three models. Are there particular digits that are frequently  confused?

## 2. Digit recognition from features of the input data

## 3. Convolutional neural network digit recognition

## 4. Chest X-ray to detect pneumonia

> Train a CNN for the chest x-ray pneumonia recognition. In order to do so, complete the code to reproduce the architecture plotted in the notebook. Present the confusion matrix, accuracy and F1-score of the validation and test datasets and discuss your results.

## General questions

### Question 1

> What is the learning algorithm being used to train the neural networks?

The algorithm used to optimize the weights is RMSprop. RMSprop is an optimization algorithm used to adjust the weights of a neural network. It adapts the learning rates of the weights by using a moving average of the squared previous gradients, which allows faster convergence and improves learning performance.

<div style="text-align:center">
    <img src="figures/rmsprop_equation.png" alt="drawing" style="width:300"/>
</div>

where:

- E[g] is the moving average of the squared gradients
- δc/δw is the gradient of the cost function with respect to the weight
- η is the learning rate
- β is the moving average parameter

> What are the parameters (arguments) being used by that algorithm?

The parameters used in the ``RMSprop`` optimizer can be found in the `Keras` documentation and are as follows:

```python
keras.optimizers.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    loss_scale_factor=None,
    gradient_accumulation_steps=None,
    name="rmsprop",
    **kwargs
)
```

> What cost function is being used? 

The cost function used is the categorical cross-entropy loss function. This function is commonly used for multi-class classification problems, where the goal is to predict the class label of an input from multiple possible classes. The categorical cross-entropy loss measures the difference between the predicted probability distribution and the true distribution (one-hot encoded labels) and penalizes incorrect predictions more heavily. The equation for the categorical cross-entropy loss is given by:

```
L = - ∑(y * log(y_pred))
```

where:
- L is the loss
- y is the true label (one-hot encoded)
- y_pred is the predicted probability distribution

> How did you create the training, validation and test datasets.

### Question 2

See in each experiment section.

### Question 3

> Do the deep neural networks have much more “capacity” (i.e., do they have more weights?) than the shallow ones? Explain with one example.

In general, deeper convolutional neural networks tend to have more weights than shallow models. This comes from the fact that deeper CNNs usually include a larger number of layers, and each convolution layer contains several filters that hold weights. However, this is not always true and depends on the specific model configuration. For example, a shallow model with several layers and many neurons may end up with more weights than a CNN that has only one convolution layer using small filters.

### Question 4

See the section for part 4.

